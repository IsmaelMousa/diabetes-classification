{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diabetes Classification Model With **PyTorch**"
      ],
      "metadata": {
        "id": "lIwQz9S1G_kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "The steps we will follow:\n",
        "\n",
        "- Load Data\n",
        "- Define PyToch Model\n",
        "- Define Loss Function and Optimizers\n",
        "- Run a Training Loop\n",
        "- Evaluate the Model\n",
        "- Make Predictions"
      ],
      "metadata": {
        "id": "QCuRlSHIHgS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "akEzncCIHI8j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "\n",
        "**Pima Indians Diabetes**:\n",
        "This dataset describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
        "\n",
        "It's used for binary classification problems `onset of diabetes as 1 or not as 0`. All the input variables that describe each patient are transformed and numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our neural network in PyTorch."
      ],
      "metadata": {
        "id": "Gx04Dt0QJW44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"/data/train.csv\"\n",
        "\n",
        "dataset = np.loadtxt(fname=fname, delimiter=\",\", dtype=np.float32)\n",
        "\n",
        "dataset[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAcoDiwmIuD8",
        "outputId": "cd733ae1-c895-43d4-fd90-a741888b9bd5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n",
              "        6.270e-01, 5.000e+01, 1.000e+00],\n",
              "       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n",
              "        3.510e-01, 3.100e+01, 0.000e+00],\n",
              "       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n",
              "        6.720e-01, 3.200e+01, 1.000e+00],\n",
              "       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n",
              "        1.670e-01, 2.100e+01, 0.000e+00],\n",
              "       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n",
              "        2.288e+00, 3.300e+01, 1.000e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We have 9 columns where from 1st to 8th are input variables, that we will represents as `X`. And the 9th column is the output variable and will be represented as `y`.\n",
        "\n",
        "**Input Variables (X)**:\n",
        "\n",
        "1. Number of times pregnant.\n",
        "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test.\n",
        "3. Diastolic blood pressure (mm Hg).\n",
        "4. Triceps skin fold thickness (mm).\n",
        "5. 2-hour serum insulin (μIU/ml).\n",
        "6. Body mass index (weight in kg/(height in m)2).\n",
        "7. Diabetes pedigree function.\n",
        "8. Age (years).\n",
        "\n",
        "**Output Variables (y)**:\n",
        "\n",
        "9. Class label (0 or 1)."
      ],
      "metadata": {
        "id": "DjDDMYxzUhA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset[:, :8]\n",
        "y = dataset[:, 8]\n",
        "\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqfz12ByK65m",
        "outputId": "5e0e5910-f4a0-4e05-a7c0-1e29c4d3ea77"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((768, 8), (768,))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.from_numpy(X)\n",
        "y = torch.from_numpy(y).reshape(-1, 1)\n",
        "\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKuYfrQPK7So",
        "outputId": "c71ac7f0-ba9f-41e0-94be-b0a8692296a5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([768, 8]), torch.Size([768, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define The Model\n",
        "\n",
        "You can piece it all together by adding each layer such that:\n",
        "\n",
        "- The model expects rows of data with 8 variables (the first argument at the first layer set to 8).\n",
        "- The first hidden layer has 12 neurons, followed by a `ReLU` activation function.\n",
        "- The second hidden layer has 8 neurons, followed by another `ReLU` activation function.\n",
        "- The output layer has one neuron, followed by a `sigmoid` activation function."
      ],
      "metadata": {
        "id": "Z2zT4bAcXr99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiabetesClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = nn.Linear(8, 200)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.hidden2 = nn.Linear(200, 100)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.output = nn.Linear(100, 1)\n",
        "        self.act_output = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.hidden1(x))\n",
        "        x = self.act2(self.hidden2(x))\n",
        "        x = self.act_output(self.output(x))\n",
        "        return x\n",
        "\n",
        "model = DiabetesClassifier()\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQlcGMGPK7iz",
        "outputId": "07153cf0-b6b8-41e0-c38c-b9ae3fc3ccad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiabetesClassifier(\n",
            "  (hidden1): Linear(in_features=8, out_features=200, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (hidden2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (act2): ReLU()\n",
            "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (act_output): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Loss Function and Optimizers (Preparation for Training)\n",
        "\n",
        "Training a network means finding the best set of weights to map inputs to outputs in our dataset.\n",
        "\n",
        "The loss function is the metric to measure the prediction’s distance to `y`. In general the loss function is the measure of the model's performance.\n",
        "\n",
        "The optimizer is the algorithm we use to adjust/update the model weights progressively to produce a better output.\n",
        "\n",
        "\n",
        "- loss function used: `BCELoss (Binary Classification Entropy Loss)` because the type of our task is a binary classification task.\n",
        "- optimizer algorithm used: `ADAM (Adaptive Moment Estimation)`."
      ],
      "metadata": {
        "id": "D7gW9Ispo2qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "LN53OroJK7td"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trianing The Model\n",
        "\n",
        "Simply speaking, the entire dataset is split into batches, and we pass the batches one by one into a model using a training loop. Once we have exhausted all the batches, we have finished one epoch. Then we can start over again with the same dataset and start the second epoch, continuing to refine the model. This process repeats until we are satisfied with the model’s output.\n",
        "\n",
        "- **Device = cuda**: to make the training faster we make the process running on the GPU (cuda).\n",
        "- **Epoch**: Passes the entire training dataset to the model once.\n",
        "- **Batch**: One or more samples passed to the model, from which the gradient descent algorithm will be executed for one iteration.\n",
        "\n",
        "**The total number of batches over many epochs is how many times you run the gradient descent to refine the model.**"
      ],
      "metadata": {
        "id": "9T1a6qsuIIkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "X = X.to(device)\n",
        "y = y.to(device)\n",
        "model.to(device)\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        X_batch = X[i:i+batch_size]\n",
        "\n",
        "        y_prediction = model(X_batch)\n",
        "        y_batch = y[i:i+batch_size]\n",
        "\n",
        "        loss = loss_function(y_prediction, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Finished epoch: {epoch}, Latest loss: {loss}\")"
      ],
      "metadata": {
        "id": "WVADtY-UK72m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcdc8cc-552a-4edf-8796-a4a008361ea7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch: 0, Latest loss: 0.3934597074985504\n",
            "Finished epoch: 1, Latest loss: 0.6608480215072632\n",
            "Finished epoch: 2, Latest loss: 0.6762295961380005\n",
            "Finished epoch: 3, Latest loss: 0.6646689176559448\n",
            "Finished epoch: 4, Latest loss: 0.5518184900283813\n",
            "Finished epoch: 5, Latest loss: 0.4350510239601135\n",
            "Finished epoch: 6, Latest loss: 0.4045760929584503\n",
            "Finished epoch: 7, Latest loss: 0.4129359722137451\n",
            "Finished epoch: 8, Latest loss: 0.3959192633628845\n",
            "Finished epoch: 9, Latest loss: 0.4066961407661438\n",
            "Finished epoch: 10, Latest loss: 0.4124729633331299\n",
            "Finished epoch: 11, Latest loss: 0.44192013144493103\n",
            "Finished epoch: 12, Latest loss: 0.43613022565841675\n",
            "Finished epoch: 13, Latest loss: 0.438364714384079\n",
            "Finished epoch: 14, Latest loss: 0.43524372577667236\n",
            "Finished epoch: 15, Latest loss: 0.4504162073135376\n",
            "Finished epoch: 16, Latest loss: 0.4320225417613983\n",
            "Finished epoch: 17, Latest loss: 0.44153255224227905\n",
            "Finished epoch: 18, Latest loss: 0.4431651532649994\n",
            "Finished epoch: 19, Latest loss: 0.45669159293174744\n",
            "Finished epoch: 20, Latest loss: 0.4016580283641815\n",
            "Finished epoch: 21, Latest loss: 0.4357869029045105\n",
            "Finished epoch: 22, Latest loss: 0.44579973816871643\n",
            "Finished epoch: 23, Latest loss: 0.4438629746437073\n",
            "Finished epoch: 24, Latest loss: 0.447399765253067\n",
            "Finished epoch: 25, Latest loss: 0.45481815934181213\n",
            "Finished epoch: 26, Latest loss: 0.4395335912704468\n",
            "Finished epoch: 27, Latest loss: 0.4391346275806427\n",
            "Finished epoch: 28, Latest loss: 0.43291354179382324\n",
            "Finished epoch: 29, Latest loss: 0.4298374652862549\n",
            "Finished epoch: 30, Latest loss: 0.4480881989002228\n",
            "Finished epoch: 31, Latest loss: 0.44755953550338745\n",
            "Finished epoch: 32, Latest loss: 0.4230019450187683\n",
            "Finished epoch: 33, Latest loss: 0.44753238558769226\n",
            "Finished epoch: 34, Latest loss: 0.43672746419906616\n",
            "Finished epoch: 35, Latest loss: 0.42666861414909363\n",
            "Finished epoch: 36, Latest loss: 0.4322551488876343\n",
            "Finished epoch: 37, Latest loss: 0.4345359206199646\n",
            "Finished epoch: 38, Latest loss: 0.43947267532348633\n",
            "Finished epoch: 39, Latest loss: 0.4257218539714813\n",
            "Finished epoch: 40, Latest loss: 0.43535929918289185\n",
            "Finished epoch: 41, Latest loss: 0.46201932430267334\n",
            "Finished epoch: 42, Latest loss: 0.4127608835697174\n",
            "Finished epoch: 43, Latest loss: 0.434554785490036\n",
            "Finished epoch: 44, Latest loss: 0.4083428978919983\n",
            "Finished epoch: 45, Latest loss: 0.39977237582206726\n",
            "Finished epoch: 46, Latest loss: 0.4052833914756775\n",
            "Finished epoch: 47, Latest loss: 0.4433213770389557\n",
            "Finished epoch: 48, Latest loss: 0.4318675994873047\n",
            "Finished epoch: 49, Latest loss: 0.43785232305526733\n",
            "Finished epoch: 50, Latest loss: 0.4223540723323822\n",
            "Finished epoch: 51, Latest loss: 0.4298820197582245\n",
            "Finished epoch: 52, Latest loss: 0.4160301089286804\n",
            "Finished epoch: 53, Latest loss: 0.4321880340576172\n",
            "Finished epoch: 54, Latest loss: 0.4210580587387085\n",
            "Finished epoch: 55, Latest loss: 0.4338798522949219\n",
            "Finished epoch: 56, Latest loss: 0.4115527272224426\n",
            "Finished epoch: 57, Latest loss: 0.41448333859443665\n",
            "Finished epoch: 58, Latest loss: 0.42050406336784363\n",
            "Finished epoch: 59, Latest loss: 0.4161853790283203\n",
            "Finished epoch: 60, Latest loss: 0.4000210165977478\n",
            "Finished epoch: 61, Latest loss: 0.40760576725006104\n",
            "Finished epoch: 62, Latest loss: 0.37681710720062256\n",
            "Finished epoch: 63, Latest loss: 0.3881274461746216\n",
            "Finished epoch: 64, Latest loss: 0.39865708351135254\n",
            "Finished epoch: 65, Latest loss: 0.3740660846233368\n",
            "Finished epoch: 66, Latest loss: 0.3758377432823181\n",
            "Finished epoch: 67, Latest loss: 0.3449900448322296\n",
            "Finished epoch: 68, Latest loss: 0.36277270317077637\n",
            "Finished epoch: 69, Latest loss: 0.3507079780101776\n",
            "Finished epoch: 70, Latest loss: 0.35915374755859375\n",
            "Finished epoch: 71, Latest loss: 0.3495435118675232\n",
            "Finished epoch: 72, Latest loss: 0.3464794158935547\n",
            "Finished epoch: 73, Latest loss: 0.34205713868141174\n",
            "Finished epoch: 74, Latest loss: 0.3454495668411255\n",
            "Finished epoch: 75, Latest loss: 0.35553407669067383\n",
            "Finished epoch: 76, Latest loss: 0.34650981426239014\n",
            "Finished epoch: 77, Latest loss: 0.34605926275253296\n",
            "Finished epoch: 78, Latest loss: 0.3389597237110138\n",
            "Finished epoch: 79, Latest loss: 0.34789958596229553\n",
            "Finished epoch: 80, Latest loss: 0.33201754093170166\n",
            "Finished epoch: 81, Latest loss: 0.3333423435688019\n",
            "Finished epoch: 82, Latest loss: 0.3433528542518616\n",
            "Finished epoch: 83, Latest loss: 0.33582690358161926\n",
            "Finished epoch: 84, Latest loss: 0.3290005028247833\n",
            "Finished epoch: 85, Latest loss: 0.32108983397483826\n",
            "Finished epoch: 86, Latest loss: 0.4599049687385559\n",
            "Finished epoch: 87, Latest loss: 0.3642478883266449\n",
            "Finished epoch: 88, Latest loss: 0.31940457224845886\n",
            "Finished epoch: 89, Latest loss: 0.31324630975723267\n",
            "Finished epoch: 90, Latest loss: 0.304199755191803\n",
            "Finished epoch: 91, Latest loss: 0.3115037679672241\n",
            "Finished epoch: 92, Latest loss: 0.3194139897823334\n",
            "Finished epoch: 93, Latest loss: 0.298833966255188\n",
            "Finished epoch: 94, Latest loss: 0.3141954839229584\n",
            "Finished epoch: 95, Latest loss: 0.3001498281955719\n",
            "Finished epoch: 96, Latest loss: 0.30318906903266907\n",
            "Finished epoch: 97, Latest loss: 0.29752117395401\n",
            "Finished epoch: 98, Latest loss: 0.30436545610427856\n",
            "Finished epoch: 99, Latest loss: 0.2917045056819916\n",
            "Finished epoch: 100, Latest loss: 0.29560336470603943\n",
            "Finished epoch: 101, Latest loss: 0.28122368454933167\n",
            "Finished epoch: 102, Latest loss: 0.2910872995853424\n",
            "Finished epoch: 103, Latest loss: 0.2977204918861389\n",
            "Finished epoch: 104, Latest loss: 0.2681858539581299\n",
            "Finished epoch: 105, Latest loss: 0.2978528141975403\n",
            "Finished epoch: 106, Latest loss: 0.28327590227127075\n",
            "Finished epoch: 107, Latest loss: 0.2650162875652313\n",
            "Finished epoch: 108, Latest loss: 0.27320945262908936\n",
            "Finished epoch: 109, Latest loss: 0.2559911608695984\n",
            "Finished epoch: 110, Latest loss: 0.24705161154270172\n",
            "Finished epoch: 111, Latest loss: 0.24819402396678925\n",
            "Finished epoch: 112, Latest loss: 0.21605432033538818\n",
            "Finished epoch: 113, Latest loss: 0.24552899599075317\n",
            "Finished epoch: 114, Latest loss: 0.22777171432971954\n",
            "Finished epoch: 115, Latest loss: 0.19301944971084595\n",
            "Finished epoch: 116, Latest loss: 0.19405898451805115\n",
            "Finished epoch: 117, Latest loss: 0.18579262495040894\n",
            "Finished epoch: 118, Latest loss: 0.1979019194841385\n",
            "Finished epoch: 119, Latest loss: 0.16792529821395874\n",
            "Finished epoch: 120, Latest loss: 0.18765950202941895\n",
            "Finished epoch: 121, Latest loss: 0.17035716772079468\n",
            "Finished epoch: 122, Latest loss: 0.1729360818862915\n",
            "Finished epoch: 123, Latest loss: 0.16465389728546143\n",
            "Finished epoch: 124, Latest loss: 0.1699465662240982\n",
            "Finished epoch: 125, Latest loss: 0.17488595843315125\n",
            "Finished epoch: 126, Latest loss: 0.16331744194030762\n",
            "Finished epoch: 127, Latest loss: 0.16967564821243286\n",
            "Finished epoch: 128, Latest loss: 0.157668799161911\n",
            "Finished epoch: 129, Latest loss: 0.1656877100467682\n",
            "Finished epoch: 130, Latest loss: 0.15446525812149048\n",
            "Finished epoch: 131, Latest loss: 0.16653522849082947\n",
            "Finished epoch: 132, Latest loss: 0.15618330240249634\n",
            "Finished epoch: 133, Latest loss: 0.14369738101959229\n",
            "Finished epoch: 134, Latest loss: 0.1510569453239441\n",
            "Finished epoch: 135, Latest loss: 0.14771336317062378\n",
            "Finished epoch: 136, Latest loss: 0.17088592052459717\n",
            "Finished epoch: 137, Latest loss: 0.15487930178642273\n",
            "Finished epoch: 138, Latest loss: 0.13754907250404358\n",
            "Finished epoch: 139, Latest loss: 0.13833409547805786\n",
            "Finished epoch: 140, Latest loss: 0.14791055023670197\n",
            "Finished epoch: 141, Latest loss: 0.16540107131004333\n",
            "Finished epoch: 142, Latest loss: 0.1350281536579132\n",
            "Finished epoch: 143, Latest loss: 0.13280898332595825\n",
            "Finished epoch: 144, Latest loss: 0.10924442857503891\n",
            "Finished epoch: 145, Latest loss: 0.142363041639328\n",
            "Finished epoch: 146, Latest loss: 0.13083958625793457\n",
            "Finished epoch: 147, Latest loss: 0.13209442794322968\n",
            "Finished epoch: 148, Latest loss: 0.14040222764015198\n",
            "Finished epoch: 149, Latest loss: 0.12365748733282089\n",
            "Finished epoch: 150, Latest loss: 0.09627373516559601\n",
            "Finished epoch: 151, Latest loss: 0.12521031498908997\n",
            "Finished epoch: 152, Latest loss: 0.11561539769172668\n",
            "Finished epoch: 153, Latest loss: 0.1050960049033165\n",
            "Finished epoch: 154, Latest loss: 0.15286774933338165\n",
            "Finished epoch: 155, Latest loss: 0.12074372917413712\n",
            "Finished epoch: 156, Latest loss: 0.12228399515151978\n",
            "Finished epoch: 157, Latest loss: 0.08905430138111115\n",
            "Finished epoch: 158, Latest loss: 0.11214257776737213\n",
            "Finished epoch: 159, Latest loss: 0.11219950765371323\n",
            "Finished epoch: 160, Latest loss: 0.10129301249980927\n",
            "Finished epoch: 161, Latest loss: 0.10701519250869751\n",
            "Finished epoch: 162, Latest loss: 0.11202669143676758\n",
            "Finished epoch: 163, Latest loss: 0.09422514587640762\n",
            "Finished epoch: 164, Latest loss: 0.09616942703723907\n",
            "Finished epoch: 165, Latest loss: 0.08558399975299835\n",
            "Finished epoch: 166, Latest loss: 0.12173277139663696\n",
            "Finished epoch: 167, Latest loss: 0.09518317133188248\n",
            "Finished epoch: 168, Latest loss: 0.09083382785320282\n",
            "Finished epoch: 169, Latest loss: 0.10540434718132019\n",
            "Finished epoch: 170, Latest loss: 0.08545762300491333\n",
            "Finished epoch: 171, Latest loss: 0.0936846137046814\n",
            "Finished epoch: 172, Latest loss: 0.07920095324516296\n",
            "Finished epoch: 173, Latest loss: 0.0926622748374939\n",
            "Finished epoch: 174, Latest loss: 0.11875739693641663\n",
            "Finished epoch: 175, Latest loss: 0.10207383334636688\n",
            "Finished epoch: 176, Latest loss: 0.08726254105567932\n",
            "Finished epoch: 177, Latest loss: 0.07885005325078964\n",
            "Finished epoch: 178, Latest loss: 0.0749746561050415\n",
            "Finished epoch: 179, Latest loss: 0.10032771527767181\n",
            "Finished epoch: 180, Latest loss: 0.0850093811750412\n",
            "Finished epoch: 181, Latest loss: 0.10148952901363373\n",
            "Finished epoch: 182, Latest loss: 0.07266590744256973\n",
            "Finished epoch: 183, Latest loss: 0.06242447346448898\n",
            "Finished epoch: 184, Latest loss: 0.06852046400308609\n",
            "Finished epoch: 185, Latest loss: 0.09104767441749573\n",
            "Finished epoch: 186, Latest loss: 0.07717026770114899\n",
            "Finished epoch: 187, Latest loss: 0.10629016160964966\n",
            "Finished epoch: 188, Latest loss: 0.10886915773153305\n",
            "Finished epoch: 189, Latest loss: 0.11185318976640701\n",
            "Finished epoch: 190, Latest loss: 0.09464390575885773\n",
            "Finished epoch: 191, Latest loss: 0.10732186585664749\n",
            "Finished epoch: 192, Latest loss: 0.0945209190249443\n",
            "Finished epoch: 193, Latest loss: 0.0893888771533966\n",
            "Finished epoch: 194, Latest loss: 0.07199093699455261\n",
            "Finished epoch: 195, Latest loss: 0.0684037134051323\n",
            "Finished epoch: 196, Latest loss: 0.10425807535648346\n",
            "Finished epoch: 197, Latest loss: 0.06825001537799835\n",
            "Finished epoch: 198, Latest loss: 0.06467541307210922\n",
            "Finished epoch: 199, Latest loss: 0.07213087379932404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate The Model\n",
        "\n",
        "We can use `accuracy` as our evaluation score by converting the output (a floating point in the range of 0 to 1) to an integer (0 or 1) and compare to the label we know."
      ],
      "metadata": {
        "id": "kDBgu1-PbYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_prediction = model(X)\n",
        "\n",
        "accuracy = (y_prediction.round() == y).float().mean()\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "1bJ4HowaK8Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25b36db-15cd-464b-eb15-f8072cc32aee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Predictions\n",
        "We are using a `sigmoid` activation function on the output layer so that the predictions will be a probability in the range between **0 and 1**,\n",
        "so we can convert the probability into 0 or 1 to predict crisp classes directly by:"
      ],
      "metadata": {
        "id": "G1JJt_zVciDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = (model(X) > 0.5).int()\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"{X[i].tolist()} => {predictions[i].item()} (expected {int(y[i].item())})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX0yC66CcjyL",
        "outputId": "e3a06b91-e4f8-47e0-8674-b34f565b4834"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
            "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
            "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
            "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
            "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n",
            "[5.0, 116.0, 74.0, 0.0, 0.0, 25.600000381469727, 0.20100000500679016, 30.0] => 0 (expected 0)\n",
            "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.24799999594688416, 26.0] => 0 (expected 1)\n",
            "[10.0, 115.0, 0.0, 0.0, 0.0, 35.29999923706055, 0.1340000033378601, 29.0] => 0 (expected 0)\n",
            "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.15800000727176666, 53.0] => 1 (expected 1)\n",
            "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.23199999332427979, 54.0] => 1 (expected 1)\n"
          ]
        }
      ]
    }
  ]
}